{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"46YiWIKAm7UE"},"source":["드리아브 마운트"]},{"cell_type":"code","metadata":{"id":"Un1UuUmem7qC"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive/\", force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["기본 경로 설정"],"metadata":{"id":"fvETAmyMwt7J"}},{"cell_type":"code","metadata":{"id":"eJuHxnBziQ2I"},"source":["cd \"/content/gdrive/MyDrive/2024_MCL_Internship/CIFAR10\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksehhX3Dtbxt"},"source":["라이브러리 불러오기"]},{"cell_type":"code","metadata":{"id":"1xdH3ITXtZvu"},"source":["import os\n","from time import time\n","from matplotlib import pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ofGXMdMhwks"},"source":["데이터셋 불러오기"]},{"cell_type":"code","metadata":{"id":"3Yp-t2GNhyVF"},"source":["# data 불러오기\n","transform = transforms.Compose(\n","    [transforms.ToTensor()]\n",")\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                    download=True, transform=transform)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","        'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Datset Visualization"],"metadata":{"id":"OzWKhclpIEF1"}},{"cell_type":"code","source":["import random\n","from torch.nn.functional import interpolate\n","\n","print(\"total length of train dataset : \", len(trainset))\n","print(\"total length of test dataset : \", len(testset))\n","i = random.randrange(len(trainset))\n","image_train, label_train = trainset[i]\n","\n","image_train_PIL = transforms.ToPILImage()(interpolate(image_train.unsqueeze(0), scale_factor=4, mode=\"nearest\")[0])\n","display(image_train_PIL)\n","print(classes[label_train])\n","print()\n","\n","i = random.randrange(len(testset))\n","image_test, label_test = testset[i]\n","image_test_PIL = transforms.ToPILImage()(interpolate(image_test.unsqueeze(0), scale_factor=4, mode=\"nearest\")[0])\n","display(image_test_PIL)\n","print(classes[label_test])"],"metadata":{"id":"cIBOKUVbIDCw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NrF3feZrtV0i"},"source":["model 정의하기"]},{"cell_type":"code","metadata":{"id":"dFfoz544tUi6"},"source":["class Linear_Model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.flatten = nn.Flatten()\n","\n","        self.linear = nn.Sequential(\n","            nn.Linear(3 * 32 * 32, 2048),\n","            nn.BatchNorm1d(2048),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(2048, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, 512),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(512, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 256),\n","            nn.BatchNorm1d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(128, 64),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(64, 32),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(32, 10),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, x):\n","        logits = self.flatten(x)\n","        logits = self.linear(logits)\n","        return logits\n","\n","class vgg_model(nn.Module):\n","    def __init__(self):\n","        super(vgg_model, self).__init__()\n","        self.feature_layer1 = nn.Sequential( # conv1_1, conv1_2, MaxPool1\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), # conv1_1 : 3 x 32 x 32 -> 64 x 32 x 32\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), # conv1_2 : 64 x 32 x 32 -> 64 x 32 x 32\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2) # 64 x 16 x 16\n","        )\n","        self.feature_layer2 = nn.Sequential( # conv2_1, conv2_2, Pool2\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2) # 128 x  8 x 8\n","        )\n","        self.feature_layer3 = nn.Sequential( # conv3_1, conv3_2, conv3_3, Pool3\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2) # 256 x 4 x 4\n","        )\n","        self.feature_layer4 = nn.Sequential( # conv4_1, conv4_2, conv4_3, Pool4\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2) # 512 x 2 x 2\n","        )\n","        self.feature_layer5 = nn.Sequential( # conv5_1, conv5_2, conv5_3, Pool5\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2) # B x 512 x 1 x 1\n","        )\n","        self.classifier = nn.Sequential( # 512 -> 256 -> 100 -> 10 FC Layers, ReLU로 activation, 마지막은 softmax\n","            nn.Linear(512, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 100),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(100, 10),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, x):\n","        logits = self.feature_layer1(x)\n","        logits = self.feature_layer2(logits)\n","        logits = self.feature_layer3(logits)\n","        logits = self.feature_layer4(logits)\n","        logits = self.feature_layer5(logits)\n","        logits = logits.view(logits.size(0), -1)\n","        logits = self.classifier(logits)\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"RWoUr3EnL00F"}},{"cell_type":"code","source":["from torchsummary import summary as summ\n","\n","model_linear = Linear_Model().cuda()\n","model_vgg = vgg_model().cuda()\n","\n","summ(model_linear, (3, 32, 32))\n","summ(model_vgg, (3, 32, 32))"],"metadata":{"id":"AkoWneJvL0gV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터셋 sampling"],"metadata":{"id":"Q37L1iq2uCLW"}},{"cell_type":"code","source":["class CIFAR10_sampling(torch.utils.data.Dataset):\n","    def __init__(self, dataset, rate):\n","        self.img_list = []\n","        self.label_list = []\n","        cnt_list = [0] * 10\n","        for img, label in dataset:\n","            if cnt_list[label] < int(len(dataset) // 10 * rate):\n","                self.img_list.append(img)\n","                self.label_list.append(label)\n","                cnt_list[label] += 1\n","\n","    def __getitem__(self, idx):\n","        return self.img_list[idx], self.label_list[idx]\n","\n","    def __len__(self):\n","        return len(self.label_list)"],"metadata":{"id":"3S5fjYrXt_Xb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FXlSc_VKvoqd"},"source":["train 코드 정의"]},{"cell_type":"code","metadata":{"id":"sHTGtmJGvsaq"},"source":["def train_model(info):\n","    # model 정의\n","    if info[\"model\"] == \"vgg\":\n","        model = vgg_model()\n","    elif info[\"model\"] == \"linear\":\n","        model = Linear_Model()\n","    else:\n","        print(\"Model Error\")\n","        exit(0)\n","    model.cuda()\n","\n","    # checkpoints 저장 디렉토리 만들기, tensorboard 정의\n","    ckpt_path = info[\"model\"]\n","    if not os.path.exists(ckpt_path): os.mkdir(ckpt_path)\n","    pth_path = os.path.join(ckpt_path, \"pth\")\n","    if not os.path.exists(pth_path): os.mkdir(pth_path)\n","    writer = info[\"writer\"]\n","\n","    # dataset\n","    transform = transforms.Compose(\n","        [transforms.ToTensor()]\n","    )\n","    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                    download=True, transform=transform)\n","\n","    # 빠른 훈련을 위해 dataset을 일부 sampling하여 사용하고 싶은 경우\n","    trainset = CIFAR10_sampling(trainset, info[\"train_sampling_rate\"])\n","\n","    print(f\"Length of train dataset: {len(trainset)} / {int(len(trainset) / info['train_sampling_rate'])}\")\n","\n","    train_loader = torch.utils.data.DataLoader(trainset, batch_size=info[\"batch_size\"],\n","                                            shuffle=True, num_workers=2)\n","    test_loader = torch.utils.data.DataLoader(testset, batch_size=info[\"batch_size\"],\n","                                            shuffle=False, num_workers=2)\n","\n","    # Loss 정의, Optimizer, Scheduler\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=info[\"lr\"])\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n","\n","    # train, test\n","    log_loss_train = []\n","    log_loss_test = []\n","    log_acc_test = []\n","\n","    for epoch in range(info[\"epochs\"]):\n","        # train\n","        start_time = time()\n","        train_loss_per_epoch = 0\n","        model.train()\n","\n","        for iter, batch in enumerate(train_loader):\n","            image = batch[0].cuda()\n","            label = batch[1].cuda()\n","\n","            y = model(image) # size of y : Batch x 10\n","            loss_train = criterion(y, label)\n","\n","            optimizer.zero_grad()\n","            loss_train.backward()\n","            optimizer.step()\n","\n","            train_loss_per_epoch += loss_train.item() / len(train_loader)\n","\n","            writer.add_scalar(f\"{info['model']}/Train_Loss\", loss_train, epoch * len(train_loader) + iter)\n","            # if (iter + 1) % 10 == 0:\n","            #     print(f\"\\t[{iter + 1}/{len(train_loader)}] Train Loss: {loss_train.item():.4f}\")\n","\n","        print(f\"Epoch[{epoch:3d}] Train Loss: {train_loss_per_epoch:.4f}\", end='')\n","        log_loss_train.append(train_loss_per_epoch)\n","\n","        # test\n","        model.eval()\n","        loss_test_epoch = 0\n","        total_num_accs = 0\n","\n","        with torch.no_grad():\n","            for iter, batch in enumerate(test_loader):\n","                image = batch[0].cuda()\n","                label = batch[1].cuda()\n","\n","                y = model(image)\n","                loss_test = criterion(y, label)\n","                loss_test_epoch += loss_test / len(test_loader)\n","\n","                y_ = torch.argmax(y, dim=1)\n","                num_accs = torch.sum(y_ == label).item()\n","                total_num_accs += num_accs\n","\n","        acc_rate = (total_num_accs / len(testset)) * 100\n","        writer.add_scalar(f\"{info['model']}/Test_Loss\", loss_test_epoch, epoch)\n","        writer.add_scalar(f\"{info['model']}/Test_Accuracy\", acc_rate, epoch)\n","        log_loss_test.append(loss_test_epoch.item())\n","        log_acc_test.append(acc_rate)\n","\n","        # if (epoch + 1) % 10 == 0:\n","        #     torch.save({'epoch': epoch,\n","        #                 'model_state_dict': model.state_dict(),\n","        #                 'optimizer_state_dict': optimizer.state_dict()}, f'{pth_path}/Epoch{epoch:03d}.pth')\n","\n","        time_per_epoch = time() - start_time\n","        print(f\" Test Loss: {loss_test_epoch:.4f}\"\n","              f\" Accuracy Rate: {acc_rate:.2f}%\"\n","              f\" lr: {scheduler.get_last_lr()[0]:.1E}\"\n","              f\" time per epoch: {time_per_epoch:.2f}sec\"\n","              )\n","\n","        scheduler.step()\n","\n","    return log_loss_train, log_loss_test, log_acc_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TalWLzXb3vqo"},"source":["main 함수"]},{"cell_type":"code","metadata":{"id":"LqDYsouF3xGs"},"source":["if not os.path.exists('logs'): os.mkdir('logs')\n","writer = SummaryWriter('logs')\n","\n","info = {\n","    \"epochs\" : 15,\n","    \"lr\" : 0.0001,\n","    \"model\" : \"linear\",\n","    \"writer\" : writer,\n","    \"train_sampling_rate\": 0.4,\n","    \"batch_size\": 500,\n","}\n","\n","print(f\"mode: {info['model']} | epochs: {info['epochs']} | learning rate: {info['lr']}\")\n","my_cnn = train_model(info)\n","\n","info[\"model\"] = \"vgg\"\n","print(f\"\\nmode: {info['model']} | epochs: {info['epochs']} | learning rate: {info['lr']}\")\n","vgg = train_model(info)\n","\n","print(f\"Train End\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YcwLMFCv4Lyh"},"source":["pyplot"]},{"cell_type":"code","metadata":{"id":"3mjk_hpr4MF2"},"source":["# --- Logs Visualization\n","# training loss\n","plt.plot(my_cnn[0], label='Linear_Model')\n","plt.plot(vgg[0], label='VGG16')\n","plt.xticks(list(range(0, info[\"epochs\"], info[\"epochs\"] // 5)))\n","plt.legend()\n","plt.savefig('train_loss.png')\n","plt.clf()\n","\n","# test loss\n","plt.plot(my_cnn[1], label='Linear_Model')\n","plt.plot(vgg[1], label='VGG16')\n","plt.xticks(list(range(0, info[\"epochs\"], info[\"epochs\"] // 5)))\n","plt.legend()\n","plt.savefig('test_loss.png')\n","plt.clf()\n","\n","# test accuracy\n","plt.plot(my_cnn[2], label='Linear_Model')\n","plt.plot(vgg[2], label='VGG16')\n","plt.xticks(list(range(0, info[\"epochs\"], info[\"epochs\"] // 5)))\n","plt.legend()\n","plt.savefig('test_acc.png')\n","plt.clf()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUtDLH8W7yQe"},"source":["tensorboard 실행"]},{"cell_type":"code","metadata":{"id":"8a2lsjkt7z6m"},"source":["%load_ext tensorboard\n","\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]}]}